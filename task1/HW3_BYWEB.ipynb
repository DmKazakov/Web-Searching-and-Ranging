{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import lxml.etree as et\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from query import *\n",
    "from vector import *\n",
    "\n",
    "INDEX = \"ind\"\n",
    "mystem = Mystem(disambiguation=False)\n",
    "\n",
    "\n",
    "def create_action(doc_id, doc_json):\n",
    "    return {\n",
    "        '_index': INDEX,\n",
    "        '_id': doc_id,\n",
    "        '_source': doc_json\n",
    "    }\n",
    "\n",
    "\n",
    "def action_generator():\n",
    "    DOCS_FOLDER = \"documents\"\n",
    "    for filename in os.listdir(DOCS_FOLDER):\n",
    "        name = DOCS_FOLDER + os.sep + filename\n",
    "        zip_file = zipfile.ZipFile(name, 'r')\n",
    "\n",
    "        #сnt = 0\n",
    "        for filename in zip_file.filelist:\n",
    "            #if cnt == 100:\n",
    "            #    return \n",
    "            try:\n",
    "                #cnt += 1\n",
    "                doc_string = zip_file.read(filename).decode('utf-8')\n",
    "                doc_json = json.loads(doc_string)\n",
    "                doc_id = filename.orig_filename.strip(\".txt\")\n",
    "                url_to_id[doc_json['url']] = doc_id\n",
    "                doc_json['pagerank'] = pageranks[doc_id]\n",
    "                yield create_action(doc_id, json.dumps(doc_json))\n",
    "            except:\n",
    "                print(filename.orig_filename)\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'stemmed': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'russian_stemmed'\n",
    "            },\n",
    "            'titles': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'russian_stemmed'\n",
    "            },\n",
    "            'url': {\n",
    "                'type': 'text'\n",
    "            },\n",
    "            'pagerank': {\n",
    "                'type': 'rank_feature'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'russian_stemmed': {\n",
    "                    'char_filter': ['yo'],\n",
    "                    'tokenizer': 'whitespace',\n",
    "                    'filter': ['lowercase']\n",
    "                }\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'yo': {\n",
    "                    'type': 'mapping',\n",
    "                    'mappings': ['ё => е']\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'alphanum': {\n",
    "                    'type': 'char_group',\n",
    "                    'tokenize_on_chars': [\"whitespace\", \"punctuation\", \"symbol\", \"\\n\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'index': {\n",
    "            'blocks': {\n",
    "                'read_only_allow_delete': 'false'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def recreate_index():\n",
    "    try:\n",
    "        es.indices.delete(index=INDEX)\n",
    "    except:\n",
    "        pass\n",
    "    es.indices.create(index=INDEX, body=SETTINGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])\n",
    "\n",
    "\n",
    "url_to_id = {}\n",
    "pageranks = {}\n",
    "\n",
    "with open(\"pageranks.txt\", \"r\") as pagerank_file:\n",
    "    line = pagerank_file.readline()\n",
    "    while line:\n",
    "        doc_id, pr = line.strip().split(\":\")\n",
    "        pageranks[doc_id] = pr\n",
    "        line = pagerank_file.readline()\n",
    "\n",
    "recreate_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for ok, result in parallel_bulk(es, action_generator(), queue_size=4, thread_count=4, chunk_size=500):\n",
    "    if not ok:\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries:  29231\n"
     ]
    }
   ],
   "source": [
    "QUERIES_FILE = \"web2008_adhoc.xml\"\n",
    "RELEVANCE_FILE_2009 = \"or_relevant-minus_table2009.xml\"\n",
    "RELEVANCE_FILE_2008 = \"or_relevant-minus_table2008.xml\"\n",
    "queries = {}\n",
    "root = et.parse(QUERIES_FILE).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    text = element[0].text\n",
    "    id = element.attrib.get('id')\n",
    "    element.clear()\n",
    "    queries[id] = Query(id, text)\n",
    "root = et.parse(RELEVANCE_FILE_2008).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    id = element.attrib.get('id')\n",
    "    for document in element.iterfind('document', namespaces=root.nsmap):\n",
    "        if document.attrib.get('id') in url_to_id:\n",
    "            doc_id = url_to_id[document.attrib.get('id')]\n",
    "            relevance = document.attrib.get('relevance')\n",
    "            document.clear()\n",
    "            if relevance == 'vital':\n",
    "                queries[id].relevant_train.add(doc_id)\n",
    "            else:\n",
    "                queries[id].irrelevant_train.add(doc_id)\n",
    "    element.clear()\n",
    "\n",
    "root = et.parse(RELEVANCE_FILE_2009).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    id = element.attrib.get('id')\n",
    "    if len(queries[id].relevant_train) > 0 or len(queries[id].irrelevant_train) > 0:\n",
    "        continue\n",
    "    for document in element.iterfind('document', namespaces=root.nsmap):\n",
    "        doc_id = document.attrib.get('id')\n",
    "        relevance = document.attrib.get('relevance')\n",
    "        document.clear()\n",
    "        if relevance == 'vital':\n",
    "            queries[id].relevant_test.add(doc_id)\n",
    "        else:    \n",
    "            queries[id].irrelevant_test.add(doc_id)\n",
    "    element.clear()    \n",
    "\n",
    "\n",
    "print(\"Total number of queries: \", len(queries)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_text, relevant_docs, irrelevant_docs, query_result_size=100, scroll_on=False):\n",
    "    \n",
    "    ids = list(relevant_docs)\n",
    "    ids.extend(list(irrelevant_docs))\n",
    "\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'should': [\n",
    "                    {\n",
    "                        'match': {\n",
    "                            'stemmed': {\n",
    "                                'query': query_text,\n",
    "                                'boost': '5.0'\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        'match': {\n",
    "                            'titles': {\n",
    "                                'query': query_text\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"filter\": [\n",
    "                    {\n",
    "                        \"ids\": {\n",
    "                            \"values\": ids\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "\n",
    "            }\n",
    "\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    if not scroll_on:    \n",
    "        query_result = es.search(index=INDEX, body=query, size=query_result_size)\n",
    "        return query_result['hits']['hits']    \n",
    "    else:\n",
    "        result = []\n",
    "        query_result = es.search(index=INDEX, body=query, scroll='10m', size=query_result_size)\n",
    "        scroll_id = query_result['_scroll_id']\n",
    "        scroll_size = len(query_result['hits']['hits'])\n",
    "        while scroll_size > 0:\n",
    "            result.extend(query_result['hits']['hits'])\n",
    "            query_result = es.scroll(scroll_id=scroll_id, scroll='10m')\n",
    "            scroll_id = query_result['_scroll_id']\n",
    "            scroll_size = len(query_result['hits']['hits'])\n",
    "\n",
    "        return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def count_query_coverage(query_words, text):\n",
    "    matches = 0\n",
    "    for word in query_words:\n",
    "        matches += 1 if word in text else 0\n",
    "    return matches / len(query_words)\n",
    "\n",
    "\n",
    "def count_span(query_words, text_words):\n",
    "    if len(query_words) > len(text_words):\n",
    "        return 0\n",
    "    \n",
    "    words_cnt = {}\n",
    "    for word in query_words:\n",
    "        words_cnt[word] = 0\n",
    "    \n",
    "    query_words = set(query_words)\n",
    "    unique_words_cnt = 0\n",
    "    left = 0\n",
    "    min_span = len(text_words) + 1\n",
    "    for right in range(0, len(text_words)):\n",
    "        word = text_words[right]\n",
    "        if word in words_cnt:\n",
    "            words_cnt[word] += 1\n",
    "            if words_cnt[word] == 1:\n",
    "                unique_words_cnt += 1\n",
    "        \n",
    "        if unique_words_cnt == len(query_words):\n",
    "            word = text_words[left]\n",
    "            while word not in words_cnt or words_cnt[word] > 1:\n",
    "                left += 1\n",
    "                if word in words_cnt:\n",
    "                    words_cnt[word] -= 1\n",
    "                word = text_words[left]\n",
    "            min_span = min(min_span, right - left + 1)\n",
    "            \n",
    "    if min_span == len(text_words) + 1:\n",
    "        return 0\n",
    "    return len(query_words) / min_span\n",
    "\n",
    "\n",
    "def get_vectors(query_text, relevant_docs, irrelevant_docs, scroll_on=False):\n",
    "    lemmatized_query = mystem.lemmatize(query_text)\n",
    "    query_words = [word.lower() for word in lemmatized_query if word.isalnum()]\n",
    "    lemmatized_query = \" \".join(query_words)\n",
    "    result = search(lemmatized_query, relevant_docs, irrelevant_docs, 100, scroll_on)\n",
    "    vectors = []\n",
    "    for doc in result:\n",
    "        relevant = 0\n",
    "        if doc['_id'] in relevant_docs:\n",
    "            relevant = 1\n",
    "        elif len(irrelevant_docs) == 0 or doc['_id'] in irrelevant_docs:    \n",
    "            relevant = 0\n",
    "        else:\n",
    "            continue\n",
    "        bm25_score = doc['_score']\n",
    "        title_match = count_query_coverage(query_words, doc['_source']['titles'])\n",
    "        content_match = count_query_coverage(query_words, doc['_source']['stemmed'])\n",
    "        span = count_span(query_words, doc['_source']['stemmed'])\n",
    "        query_length = len(lemmatized_query)\n",
    "        doc_length = sum([len(word) for word in doc['_source']['stemmed']])\n",
    "        doc_pr = doc['_source']['pagerank']\n",
    "        url_len = len(doc['_source']['url'])\n",
    "        \n",
    "        vector = [relevant, bm25_score, title_match, content_match, span, query_length, doc_length, doc_pr, url_len]\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "\n",
    "def vector_to_string(array):\n",
    "    result = f'{int(array[0])}'\n",
    "    for i in range(1, len(array)):\n",
    "        result += f' {i}:{array[i]}'\n",
    "    return result\n",
    "\n",
    "\n",
    "def write_vectors(vectors, file_name):\n",
    "    #vectors = RobustScaler().fit_transform(np.asarray(vectors))\n",
    "    for i in range(len(vectors)):\n",
    "        vectors[i][0] = vectors[i].relevant\n",
    "    with open(file_name, 'a') as file:\n",
    "        for vector in vectors:\n",
    "            file.write(vector_to_string(vector) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vectors = []\n",
    "test_vectors = []\n",
    "test_qids = []\n",
    "train_qids = []\n",
    "for query_id in queries:\n",
    "    query = queries[query_id]\n",
    "    if len(query.relevant_test) > 0:\n",
    "        test_vectors.extend(get_vectors(query.text, query.relevant_test, query.irrelevant_test))\n",
    "    if len(query.relevant_train) > 0:\n",
    "        train_vectors.extend(get_vectors(query.text, query.relevant_train, query.irrelevant_train, scroll_on=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_vectors(train_vectors, \"train.txt\")\n",
    "write_vectors(test_vectors, \"test.txt\")\n",
    "# Rank here\n",
    "\n",
    "for ignored_feature in range(1, len(train_vectors[0])):\n",
    "    print(f'Ignored feature: {ignored_feature}')\n",
    "    write_vectors([vector[:ignored_feature] + vector[ignored_feature + 1:] for vector in train_vectors], \"train.txt\")\n",
    "    write_vectors([vector[:ignored_feature] + vector[ignored_feature + 1:] for vector in test_vectors], \"test.txt\")\n",
    "    # Rank here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
