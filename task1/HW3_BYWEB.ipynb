{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "import lxml.etree as et\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import parallel_bulk\n",
    "\n",
    "from document import *\n",
    "from query import *\n",
    "from vector import *\n",
    "\n",
    "INDEX = \"ind\"\n",
    "\n",
    "\n",
    "def create_action(doc_id, doc_json):\n",
    "    return {\n",
    "        '_index': INDEX,\n",
    "        '_id': doc_id,\n",
    "        '_source': doc_json\n",
    "    }\n",
    "\n",
    "\n",
    "def action_generator():\n",
    "    DOCS_FOLDER = \"documents\"\n",
    "    for filename in os.listdir(DOCS_FOLDER):\n",
    "        name = DOCS_FOLDER + os.sep + filename\n",
    "        zip_file = zipfile.ZipFile(name, 'r')\n",
    "\n",
    "        for filename in zip_file.filelist:\n",
    "            try:\n",
    "                doc_json = zip_file.read(filename).decode('utf-8')\n",
    "                yield create_action(filename.orig_filename.strip(\".txt\"), doc_json)\n",
    "            except:\n",
    "                print(filename.orig_filename)\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS = {\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'content': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'russian_plain'\n",
    "            },\n",
    "            'stemmed': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'russian_stemmed'\n",
    "            },\n",
    "            'titles': {\n",
    "                'type': 'text',\n",
    "                'analyzer': 'russian_plain'\n",
    "            },\n",
    "            'pagerank': {\n",
    "                'type': 'rank_feature'\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'settings': {\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'russian_plain': {\n",
    "                    'char_filter': ['yo'],\n",
    "                    'tokenizer': 'alphanum',\n",
    "                    'filter': ['lowercase']\n",
    "                },\n",
    "                'russian_stemmed': {\n",
    "                    'char_filter': ['yo'],\n",
    "                    'tokenizer': 'whitespace',\n",
    "                    'filter': ['lowercase']\n",
    "                }\n",
    "            },\n",
    "            'char_filter': {\n",
    "                'yo': {\n",
    "                    'type': 'mapping',\n",
    "                    'mappings': ['ั => ะต']\n",
    "                }\n",
    "            },\n",
    "            'tokenizer': {\n",
    "                'alphanum': {\n",
    "                    'type': 'char_group',\n",
    "                    'tokenize_on_chars': [\"whitespace\", \"punctuation\", \"symbol\", \"\\n\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'index': {\n",
    "            'blocks': {\n",
    "                'read_only_allow_delete': 'false'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def recreate_index():\n",
    "    try:\n",
    "        es.indices.delete(index=INDEX)\n",
    "    except:\n",
    "        pass\n",
    "    es.indices.create(index=INDEX, body=SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrecreate_index()\\nstart_indexing = time.time()\\nfor ok, result in parallel_bulk(es, action_generator(), queue_size=4, thread_count=4, chunk_size=500):\\n    if not ok:\\n        print(result)\\nprint(\"Indexing time: \", time.time() - start_indexing)\\nprint(\"Index size in bytes: \", es.indices.stats()[\\'_all\\'][\\'primaries\\'][\\'store\\'][\\'size_in_bytes\\'])\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200, 'timeout': 360, 'maxsize': 25}])\n",
    "\n",
    "\"\"\"\n",
    "recreate_index()\n",
    "start_indexing = time.time()\n",
    "for ok, result in parallel_bulk(es, action_generator(), queue_size=4, thread_count=4, chunk_size=500):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "print(\"Indexing time: \", time.time() - start_indexing)\n",
    "print(\"Index size in bytes: \", es.indices.stats()['_all']['primaries']['store']['size_in_bytes'])\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of queries:  495\n"
     ]
    }
   ],
   "source": [
    "\"\"\"pagerank_file = open(\"pageranks.txt\", \"r\")\n",
    "line = pagerank_file.readline()\n",
    "while line:\n",
    "    doc_id, pr = line.strip().split(\":\")\n",
    "    es.update(index=INDEX, id=doc_id, body={'doc': {'pagerank': pr}})\n",
    "    line = pagerank_file.readline()\n",
    "\"\"\"\n",
    "QUERIES_FILE = \"web2008_adhoc.xml\"\n",
    "RELEVANCE_FILE = \"or_relevant-minus_table.xml\"\n",
    "RELEVANCE_FILE_2008 = \"or_relevant-minus_table.xml\"\n",
    "queries = {}\n",
    "root = et.parse(QUERIES_FILE).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    text = element[0].text\n",
    "    id = element.attrib.get('id')\n",
    "    element.clear()\n",
    "    queries[id] = Query(id, text)\n",
    "root = et.parse(RELEVANCE_FILE).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    id = element.attrib.get('id')\n",
    "    for document in element.iterfind('document', namespaces=root.nsmap):\n",
    "        doc_id = document.attrib.get('id')\n",
    "        relevance = document.attrib.get('relevance')\n",
    "        document.clear()\n",
    "        if relevance == 'vital':\n",
    "            queries[id].relevant.add(doc_id)\n",
    "    element.clear()\n",
    "root = et.parse(RELEVANCE_FILE_200).getroot()\n",
    "for element in root.iterfind('task', namespaces=root.nsmap):\n",
    "    id = element.attrib.get('id')\n",
    "    for document in element.iterfind('document', namespaces=root.nsmap):\n",
    "        doc_id = document.attrib.get('id')\n",
    "        relevance = document.attrib.get('relevance')\n",
    "        document.clear()\n",
    "        if relevance == 'vital':\n",
    "            queries[id].relevant.add(doc_id)\n",
    "    element.clear()\n",
    "\n",
    "queries = {query_id: queries[query_id] for query_id in queries if len(queries[query_id].relevant) > 0}  \n",
    "print(\"Total number of queries: \", len(queries))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_text, query_result_size=100):\n",
    "    query = {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'should': {\n",
    "                    'match': {\n",
    "                        'content': query_text\n",
    "                    }\n",
    "                }\n",
    "\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    query_result = es.search(index=INDEX, body=query, size=query_result_size)\n",
    "    return query_result['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(query_text):\n",
    "    result = search(query_text, 100)\n",
    "    vectors = []\n",
    "    for doc in result:\n",
    "        bm25_score = doc['_score']\n",
    "        # TODO title content match: need to actually store plain title?\n",
    "        # TODO span\n",
    "        query_length = len(query_text)\n",
    "        doc_length = len(doc['_source']['content'])\n",
    "        doc_pr = doc['_source']['pagerank']\n",
    "        # TODO url length\n",
    "        vectors.append(Vector(bm25_score, False, 1, query_length, doc_length, doc_pr, 0))\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query_id in query get vectors for query_id and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
