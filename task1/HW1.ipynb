{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание\n",
    "\n",
    "В задании требовалось обработать коллекцию страниц в домене первого уровня .by BY.WEB. Требовалось проанализировать следующие характеристики (далее в тексте коллекция — все слова из документов, словарь — уникальные слова из документов):\n",
    "\n",
    "\n",
    "\n",
    " - Общие\n",
    "\n",
    "   - Общее количество документов\n",
    "    \n",
    " - Текстовые\n",
    " \n",
    "   - Распределение длин документов в словах и байтах\n",
    "   - Средняя длина документа в словах и байтах\n",
    "   - Соотношение объёма документа и исходной HTML-страницы\n",
    "   \n",
    " - Морфологические\n",
    " \n",
    "   - Доля стоп-слов в коллекции\n",
    "   - Доля слов латиницей в коллекции и словаре\n",
    "   - Средняя длина слова в коллекции и словаре\n",
    "   - Частота в коллекции для слов из словаря\n",
    "   - Инвертированная документная частота для слов из словаря\n",
    "\n",
    "Также требовалось построить график зависимости ранга слова от частоты в логарифмических координатах и граф гиперссылок между страницами коллекции.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание данных\n",
    "\n",
    "На вход было подано 10 файлов в формате xml, размер каждого из которых составлял около 1 Гб. Каждый файл содержал 20000 документов: содержимого документа, ссылки на документ и целочисленного идентификатора. Содержимое и ссылка были закодированы в base64, раскодированный текст кодирован в cp1251."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание методов, инструментов, подходов. Описание эксперимента\n",
    "\n",
    "Код для обработки и анализа коллекции был написан на языке Python.\n",
    "\n",
    "Для извлечения данных из xml использовалась библиотека `lxml` и её метод `iterparse`, поскольку это достаточно быстрый и простой способ обработки документа. Для декодирования из base64 использовалась библиотека `pybase64`, поскольку она быстрее стандартной `base64`. Для разбора HTML-страниц использовалась библиотека `Beautiful Soup` с парсером `lxml`, при этом из страницы вырезались комментарии и содержимое с тэгами `script` и `style`, содержащими код на языке Javascript или CSS-фрагменты. \n",
    "\n",
    "После разбора HTML-страницы получался текстовый документ, который затем разбивался на лемматизированные слова при помощи библиотеки `pymystem3`. Из полученных слов выбирались те, которые состояли букв и цифр, чтобы избежать знаков препинания и прочих символов, после чего слова приводились в нижний регистр (`mystem` обрабатывает только слова русского языка, но в тексте также встречалось некоторое количество слов латиницей).\n",
    "\n",
    "Полный текст для каждого документа не сохранялся. Вместо этого учитывалась статистика по документу: размер документа в словах и байтах, соотношение объёма текста и исходного HTML-документа, количество слов в документе, а также ссылки, содержащиеся в документе. Для каждого документа найденные слова записывались в словарь (если слово уже присутствовало в словаре, для него инкрементировались значения частоты в коллекции и документной частоты). \n",
    "\n",
    "По собранной статистике были вычислены следующие характеристики: общее количество документов, распределение длин документов в словах и байтах, средний размер документа в словах и байтах, среднее соотношение объёма текста и исходного HTML-документа. Для визуализации была выбрана библиотека `plotly`, позволяющая строить интерактивные гистограммы и графики.\n",
    "\n",
    "По словарю были вычислены следующие характеристики: средняя длина слова в коллекции и словаре, доля стоп-слов в коллекции (стоп-слова были взяты из архива `stopwords`, используемого в `nltk`), доля слов латиницей в коллекции и словаре, частота в коллекции и инвертированная документная частота. Были определены пять самых больших значений частоты в коллекции и пять самых маленьких значений инвертированной документной частоты и выведены слова, соответствующие полученным значениям. Стоп-слова при этом не учитывались. Инвертированная документная частота слова $w$ вычислялась по формуле $\\log \\frac{N}{df_w}$, где $N$ — общее количество документов, $df_w$ — документная частота слова $w$. \n",
    "\n",
    "Для построения графика зависимости ранга слова от частоты в логарифмических координатах словарь был отсортирован по частоте в коллекции. \n",
    "\n",
    "\n",
    "Для работы с гиперссылками использовалась библиотека `urllib`, позволившая удалить из ссылок параметры, а также перевести относительные ссылки в абсолютные, используя адрес документа. Для визуализации с помощью библиотеки `pyvis` по обработанным ссылкам строился граф. Размер вершины, соответствующей документу, зависит от количество ссылок на него. Для большей наглядности в граф попали документы, на которые ссылаются не менее 300 прочих документов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import lxml.etree as et\n",
    "\n",
    "from dictionary import *\n",
    "from utils import *\n",
    "from document import *\n",
    "from graph import *\n",
    "\n",
    "\n",
    "def parse_xml(filename, docs_stat):\n",
    "    context = et.iterparse(filename, tag='document')\n",
    "\n",
    "    for (_, elem) in context:\n",
    "        content = elem[0].text\n",
    "        url = elem[1].text\n",
    "        doc_id = int(elem[2].text)\n",
    "        elem.clear()\n",
    "\n",
    "        try:\n",
    "            doc = Document(decode_base64_cp1251(content), doc_id, decode_base64_cp1251(url))\n",
    "            stats = doc.calc_doc_stats()\n",
    "            docs_stat.append(stats)\n",
    "            graph.add_document(stats)\n",
    "            dictionary.add_doc_words(doc.words)\n",
    "        except:\n",
    "            print(\"Unable to parse \" + str(doc_id))\n",
    "\n",
    "\n",
    "XML_FOLDER = \"byweb_for_course\"\n",
    "\n",
    "\n",
    "docs_stat = []\n",
    "mystem = Mystem()\n",
    "dictionary = Dictionary()\n",
    "graph = LinkGraph()\n",
    "\n",
    "for filename in os.listdir(XML_FOLDER):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        parse_xml(XML_FOLDER + os.sep + filename, docs_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total documents count: \" + str(len(docs_stat)))\n",
    "print(\"Average document length: \" + float_to_str(average_doc_size_in_words(docs_stat)) + \" words\")\n",
    "print(\"Average document length: \" + float_to_str(average_doc_size_in_bytes(docs_stat)) + \" bytes\")\n",
    "print(\"Average text content to HTML content ratio: \" + float_to_str(average_doc_text_to_html_ratio(docs_stat)))\n",
    "plot_histogram(\n",
    "    data=[stat.words_cnt for stat in docs_stat],\n",
    "    title=\"Length in words distribution\",\n",
    "    x_axis_title=\"words\",\n",
    "    y_axis_title=\"documents\",\n",
    "    step=250\n",
    ")\n",
    "plot_histogram(\n",
    "    data=[stat.bytes_cnt for stat in docs_stat],\n",
    "    title=\"Length in bytes distribution\",\n",
    "    x_axis_title=\"bytes\",\n",
    "    y_axis_title=\"documents\",\n",
    "    step=5000\n",
    ")\n",
    "\n",
    "print(\"Collection stop words ratio: \" + float_to_str(dictionary.stop_words_proportion(in_collection=True)))\n",
    "print(\"Collection latin words ratio: \" + float_to_str(dictionary.latin_words_proportion(in_collection=True)))\n",
    "print(\"Dictionary latin words ratio: \" + float_to_str(dictionary.latin_words_proportion(in_collection=False)))\n",
    "print(\"Dictionary average word length: \" + float_to_str(dictionary.average_dic_word_len()))\n",
    "print(\"Collection average word length: \" + float_to_str(dictionary.average_word_len()))\n",
    "\n",
    "print(\"Words with largest collection frequency: \" +\n",
    "      str(dictionary.most_popular_word(lambda v: dictionary.dict[v].cnt)))\n",
    "print(\"Words with smallest inverse document frequency: \" +\n",
    "      str(dictionary.most_popular_word(lambda v: math.log10(len(docs_stat) / dictionary.dict[v].doc_cnt),\n",
    "                                       get_max=False)))\n",
    "\n",
    "rf = rank_frequency({word: value.cnt for word, value in dictionary.dict.items()})\n",
    "plot_line(rf, \"Rank-frequency\", \"rank\", \"frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ\n",
    "\n",
    "В коллекции преобладают небольшие документы длиной до 1000 слов (средняя длина документа — 839 слов). \n",
    "\n",
    "Средняя длина документа в коллекции получилась короче, чем в словаре, поскольку в коллекции достаточно большой (20%) процент стоп-слов, которые в массе своей довольно короткие.\n",
    "\n",
    "Текст, видимый посетителю сайта, занимает лишь малую часть документа (19%) в то время, как основной объем приходится на разметку, CSS и JS.\n",
    "\n",
    "Слов латиницей получилось неожиданно много — возможно, потому, что почти на каждой странице встречается несколько латинских слов (название сайта, ссылки и т.п.), а на некоторых довольно много (страницы с форума linux.by). \n",
    "\n",
    "График зависимости логарифма частоты слова от логарифма его ранга подтверждает закон Ципфа: каждое следующее по частоте слово встречается резко реже предыдущего.\n",
    "\n",
    "Среди наиболее часто встречающихся в документах (т.е. с наименьшей инвертированной документной частотой) слов находятся в основном такие, которые как-то описывают функциональность сайта (“поиск”, “главный” в смысле “главная страница”), также естественным образом оказалось популярно слово “беларусь”. \n",
    "\n",
    "Частыми оказались слова “url”, “org”, “www”,  “http”.  Вероятно, из-за того, что нередко ссылки оставляют простым текстом.  Также встречается слово “г”, что скорее всего является сокращением от “год”.\n",
    "\n",
    "Как ожидалось, страницы одного домена часто ссылаются друг на друга и образуют кластеры. Оказалось очень много ссылок на tut.by, в частности на catalog.tut.by/. Кроме этого можно отметить news.br.by/, cards.br.by/, photoclub.by, avto.tdj.by.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
